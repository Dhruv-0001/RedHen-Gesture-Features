{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING AUTOENCODER MODEL"
      ],
      "metadata": {
        "id": "o_odUPXlPG98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9ToWQNeH7gC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4ad8c6-6138-4d37-d667-04ed531182d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "WINDOW_SIZE = 5\n",
        "MAX_PERSONS = 6\n",
        "NO_COLS = len([\"neck_x\", \"neck_y\", \"right shoulder_x\", \"right shoulder_y\", \"right elbow_x\", \"right elbow_y\", \"right wrist_x\",\n",
        "              \"right wrist_y\", \"left shoulder_x\", \"left shoulder_y\", \"left elbow_x\", \"left elbow_y\", \"left wrist_x\", \"left wrist_y\"])\n",
        "CHANNELS = 1\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 22\n",
        "lr = 1e-3\n",
        "split1 = 0.7\n",
        "split2 = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir(\"/content/drive/MyDrive/GSoC/npy_files/\")\n",
        "random.seed(42)\n",
        "random.shuffle(files)\n",
        "files = [\"/content/drive/MyDrive/GSoC/npy_files/\"+fil for fil in files]\n",
        "samples = len(files)\n",
        "l1 = int(samples*split1)\n",
        "l2 = int(samples*split2)\n",
        "files_train, files_val, files_test = files[:l1], files[l1:l2], files[l2:]\n",
        "\n",
        "\n",
        "x_train , y_train, x_val, y_val = [], [], [], []\n",
        "\n",
        "for fil in files_train[:14]:\n",
        "    with open(fil, \"rb\") as npf:\n",
        "        data = np.load(npf, allow_pickle=True)\n",
        "    for frame, d, lb in data:\n",
        "\n",
        "        x_train.append(np.array([d], dtype=np.float32))   # 1 channel required\n",
        "        y_train.append(lb)\n",
        "\n",
        "for fil in files_val:\n",
        "    with open(fil, \"rb\") as npf:\n",
        "        data = np.load(npf, allow_pickle=True)\n",
        "    for frame, d, lb in data:\n",
        "\n",
        "        x_val.append(np.array([d], dtype=np.float32))   # 1 channel required\n",
        "        y_val.append(lb)\n",
        "\n",
        "\n",
        "x_train = np.array(x_train, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=int)\n",
        "x_val = np.array(x_val, dtype=np.float32)\n",
        "y_val = np.array(y_val, dtype=int)"
      ],
      "metadata": {
        "id": "Zh8fm9YKIJEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "\n",
        "WINDOW_SIZE = 5\n",
        "MAX_PERSONS = 6\n",
        "NO_COLS = 14  # Number of columns in your data (adjust as needed)\n",
        "CHANNELS = 1\n",
        "LATENT_DIM = 64  # Latent dimension for the encoder\n",
        "\n",
        "\n",
        "def get_encoder():\n",
        "\n",
        "    inp = layers.Input(shape=(WINDOW_SIZE, MAX_PERSONS, NO_COLS, CHANNELS))\n",
        "\n",
        "    x = layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), padding=\"same\", return_sequences=True, activation=\"relu\")(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=64, kernel_size=(3, 3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters=64, kernel_size=(3, 3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool3D(pool_size=(3, 3, 3), strides=(1, 1, 1))(x)\n",
        "\n",
        "    x = layers.Conv3D(filters=128, kernel_size=(3, 3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv3D(filters=128, kernel_size=(3, 3, 3), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "\n",
        "    latent = layers.Dense(units=LATENT_DIM, activation=\"relu\")(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inp, outputs=latent)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_decoder():\n",
        "    inp = layers.Input(shape=(LATENT_DIM,))\n",
        "    x = layers.Dense(units=128, activation=\"relu\")(inp)\n",
        "    x = layers.Dense(units=256, activation=\"relu\")(x)  # Additional layer\n",
        "    x = layers.Dense(units=512, activation=\"relu\")(x)  # Additional layer\n",
        "    x = layers.Dense(units=WINDOW_SIZE * MAX_PERSONS * NO_COLS * CHANNELS, activation=\"relu\")(x)\n",
        "    decoded = layers.Reshape((WINDOW_SIZE, MAX_PERSONS, NO_COLS, CHANNELS))(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inp, outputs=decoded)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Create the encoder-decoder model\n",
        "encoder = get_encoder()\n",
        "decoder = get_decoder()\n",
        "\n",
        "# Connect the encoder and decoder\n",
        "encoder_input = layers.Input(shape=(WINDOW_SIZE, MAX_PERSONS, NO_COLS, CHANNELS))\n",
        "encoded = encoder(encoder_input)\n",
        "decoded = decoder(encoded)\n",
        "\n",
        "# Create the combined model\n",
        "autoencoder = tf.keras.models.Model(inputs=encoder_input, outputs=decoded)\n",
        "\n",
        "# Compile the autoencoder\n",
        "# autoencoder.compile(\n",
        "#     loss='mse',  # Use mean squared error for reconstruction loss\n",
        "#     optimizer=tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# )\n",
        "\n",
        "autoencoder.compile(\n",
        "    loss='mse',  # Use mean squared error for reconstruction loss\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr)  # Adjust the learning rate\n",
        ")\n",
        "\n",
        "# Print the autoencoder summary\n",
        "print(autoencoder.summary())\n",
        "\n",
        "x_train, _, x_val, _ = [], [], [], []\n",
        "\n",
        "for fil in files_train[:14]:\n",
        "    with open(fil, \"rb\") as npf:\n",
        "        data = np.load(npf, allow_pickle=True)\n",
        "    for _, d, _ in data:\n",
        "        d = np.expand_dims(d, axis=-1)\n",
        "        x_train.append(d)\n",
        "\n",
        "for fil in files_val:\n",
        "    with open(fil, \"rb\") as npf:\n",
        "        data = np.load(npf, allow_pickle=True)\n",
        "    for _, d, _ in data:\n",
        "        d = np.expand_dims(d, axis=-1)\n",
        "        x_val.append(d)\n",
        "\n",
        "x_train = np.array(x_train, dtype=np.float32)\n",
        "x_val = np.array(x_val, dtype=np.float32)\n",
        "\n",
        "\n",
        "logs_dir = \"/content/models_dir/logs/\"\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "# Define callbacks\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\", patience=4, verbose=1, factor=0.5\n",
        ")\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=8, verbose=1, restore_best_weights=True\n",
        ")\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"/content/models_dir/best_autoencoder_model_2.h5\",\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    verbose=1,\n",
        "    save_best_only=True\n",
        ")\n",
        "# Learning rate scheduler to adjust learning rate schedule if needed\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 10:\n",
        "        return 0.001\n",
        "    elif epoch < 20:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "\n",
        "callbacks = [reduce_lr, tensorboard, early_stopping, model_checkpoint, lr_scheduler]\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(\n",
        "    x_train, x_train,  # Use x_train as both input and target for reconstruction\n",
        "    batch_size=batch_size,\n",
        "    epochs=30,\n",
        "    validation_data=(x_val, x_val),  # Use x_val as both input and target for validation\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtikO0jQTXbN",
        "outputId": "8e235b2a-fa73-4f90-ea5d-d3aebac94a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 5, 6, 14, 1)]     0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 64)                1045184   \n",
            "                                                                 \n",
            " model_1 (Functional)        (None, 5, 6, 14, 1)       388388    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,433,572\n",
            "Trainable params: 1,432,676\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 4394.7891\n",
            "Epoch 1: val_loss improved from inf to 2809.87866, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 314s 49ms/step - loss: 4394.7891 - val_loss: 2809.8787 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 2643.0203\n",
            "Epoch 2: val_loss improved from 2809.87866 to 2381.36499, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 293s 49ms/step - loss: 2642.8848 - val_loss: 2381.3650 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 2298.0125\n",
            "Epoch 3: val_loss improved from 2381.36499 to 2205.82300, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 296s 49ms/step - loss: 2298.0125 - val_loss: 2205.8230 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 2202.5186\n",
            "Epoch 4: val_loss improved from 2205.82300 to 2160.39380, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 290s 48ms/step - loss: 2202.5186 - val_loss: 2160.3938 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 2100.1606\n",
            "Epoch 5: val_loss improved from 2160.39380 to 2100.42456, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 292s 48ms/step - loss: 2100.3149 - val_loss: 2100.4246 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 2057.8992\n",
            "Epoch 6: val_loss improved from 2100.42456 to 2037.31970, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 295s 49ms/step - loss: 2057.8992 - val_loss: 2037.3197 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 2002.5802\n",
            "Epoch 7: val_loss improved from 2037.31970 to 2018.64160, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 315s 52ms/step - loss: 2002.6870 - val_loss: 2018.6416 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1974.8785\n",
            "Epoch 8: val_loss improved from 2018.64160 to 1982.53613, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 295s 49ms/step - loss: 1974.8785 - val_loss: 1982.5361 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1817.2256\n",
            "Epoch 9: val_loss improved from 1982.53613 to 1801.82861, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 311s 52ms/step - loss: 1817.2256 - val_loss: 1801.8286 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1802.1995\n",
            "Epoch 10: val_loss improved from 1801.82861 to 1770.49512, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 294s 49ms/step - loss: 1802.1072 - val_loss: 1770.4951 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1764.6901\n",
            "Epoch 11: val_loss improved from 1770.49512 to 1746.10046, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 294s 49ms/step - loss: 1764.5851 - val_loss: 1746.1005 - lr: 1.0000e-04\n",
            "Epoch 12/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1759.0858\n",
            "Epoch 12: val_loss improved from 1746.10046 to 1742.88513, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 295s 49ms/step - loss: 1759.0931 - val_loss: 1742.8851 - lr: 1.0000e-04\n",
            "Epoch 13/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1756.4553\n",
            "Epoch 13: val_loss did not improve from 1742.88513\n",
            "6027/6027 [==============================] - 299s 50ms/step - loss: 1756.4553 - val_loss: 1743.9176 - lr: 1.0000e-04\n",
            "Epoch 14/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1754.1904\n",
            "Epoch 14: val_loss improved from 1742.88513 to 1740.63794, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 306s 51ms/step - loss: 1754.1904 - val_loss: 1740.6379 - lr: 1.0000e-04\n",
            "Epoch 15/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1703.8464\n",
            "Epoch 15: val_loss improved from 1740.63794 to 1691.59753, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 305s 51ms/step - loss: 1703.8464 - val_loss: 1691.5975 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1699.5815\n",
            "Epoch 16: val_loss improved from 1691.59753 to 1690.45374, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 296s 49ms/step - loss: 1699.5815 - val_loss: 1690.4537 - lr: 1.0000e-04\n",
            "Epoch 17/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1697.7268\n",
            "Epoch 17: val_loss improved from 1690.45374 to 1688.45508, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 293s 49ms/step - loss: 1697.7268 - val_loss: 1688.4551 - lr: 1.0000e-04\n",
            "Epoch 18/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1696.3508\n",
            "Epoch 18: val_loss improved from 1688.45508 to 1688.40930, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 290s 48ms/step - loss: 1696.3508 - val_loss: 1688.4093 - lr: 1.0000e-04\n",
            "Epoch 19/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1695.0352\n",
            "Epoch 19: val_loss improved from 1688.40930 to 1687.80627, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 289s 48ms/step - loss: 1695.0352 - val_loss: 1687.8063 - lr: 1.0000e-04\n",
            "Epoch 20/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1693.8303\n",
            "Epoch 20: val_loss improved from 1687.80627 to 1685.70398, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 292s 48ms/step - loss: 1693.6525 - val_loss: 1685.7040 - lr: 1.0000e-04\n",
            "Epoch 21/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1689.8499\n",
            "Epoch 21: val_loss did not improve from 1685.70398\n",
            "6027/6027 [==============================] - 286s 47ms/step - loss: 1690.0443 - val_loss: 1685.7728 - lr: 1.0000e-05\n",
            "Epoch 22/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1689.5323\n",
            "Epoch 22: val_loss improved from 1685.70398 to 1685.01062, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 288s 48ms/step - loss: 1689.5323 - val_loss: 1685.0106 - lr: 1.0000e-05\n",
            "Epoch 23/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1689.3755\n",
            "Epoch 23: val_loss did not improve from 1685.01062\n",
            "6027/6027 [==============================] - 290s 48ms/step - loss: 1689.3755 - val_loss: 1685.0800 - lr: 1.0000e-05\n",
            "Epoch 24/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1689.0991\n",
            "Epoch 24: val_loss improved from 1685.01062 to 1683.69324, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 285s 47ms/step - loss: 1689.0498 - val_loss: 1683.6932 - lr: 1.0000e-05\n",
            "Epoch 25/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1689.0033\n",
            "Epoch 25: val_loss did not improve from 1683.69324\n",
            "6027/6027 [==============================] - 288s 48ms/step - loss: 1689.0033 - val_loss: 1685.2390 - lr: 1.0000e-05\n",
            "Epoch 26/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1688.7817\n",
            "Epoch 26: val_loss did not improve from 1683.69324\n",
            "6027/6027 [==============================] - 286s 48ms/step - loss: 1688.6986 - val_loss: 1686.7036 - lr: 1.0000e-05\n",
            "Epoch 27/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1688.5702\n",
            "Epoch 27: val_loss improved from 1683.69324 to 1683.48425, saving model to /content/models_dir/best_autoencoder_model_2.h5\n",
            "6027/6027 [==============================] - 289s 48ms/step - loss: 1688.5271 - val_loss: 1683.4843 - lr: 1.0000e-05\n",
            "Epoch 28/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1688.4760\n",
            "Epoch 28: val_loss did not improve from 1683.48425\n",
            "6027/6027 [==============================] - 291s 48ms/step - loss: 1688.5094 - val_loss: 1689.4341 - lr: 1.0000e-05\n",
            "Epoch 29/30\n",
            "6026/6027 [============================>.] - ETA: 0s - loss: 1688.3149\n",
            "Epoch 29: val_loss did not improve from 1683.48425\n",
            "6027/6027 [==============================] - 292s 49ms/step - loss: 1688.4072 - val_loss: 1684.7560 - lr: 1.0000e-05\n",
            "Epoch 30/30\n",
            "6027/6027 [==============================] - ETA: 0s - loss: 1688.1132\n",
            "Epoch 30: val_loss did not improve from 1683.48425\n",
            "6027/6027 [==============================] - 289s 48ms/step - loss: 1688.1132 - val_loss: 1683.8479 - lr: 1.0000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x78a87e002170>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.save(\"/content/models_dir/best_autoencoder_model_2.h5\")\n"
      ],
      "metadata": {
        "id": "YyMUXC5ZHCZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = autoencoder.evaluate(x_val, x_val, verbose=0)\n",
        "print(\"Validation Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pARw0jbmI2tf",
        "outputId": "8ecc809f-d036-4fb6-8514-6bf95e73402b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Mean Squared Error: 1683.8480224609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GSUZJiEEsXol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation"
      ],
      "metadata": {
        "id": "sM-DQKYEO8MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "WINDOW_SIZE = 5\n",
        "MAX_PERSONS = 6\n",
        "NO_COLS = 14\n",
        "CHANNELS = 1\n",
        "LATENT_DIM = 64\n",
        "num_classes = 6\n",
        "\n",
        "x_train, _, x_val, _ = [], [], [], []\n",
        "\n",
        "for fil in files_train[:14]:\n",
        "    with open(fil, \"rb\") as npf:\n",
        "        data = np.load(npf, allow_pickle=True)\n",
        "    for _, d, _ in data:\n",
        "        d = np.expand_dims(d, axis=-1)\n",
        "        x_train.append(d)\n",
        "\n",
        "for fil in files_val:\n",
        "    with open(fil, \"rb\") as npf:\n",
        "        data = np.load(npf, allow_pickle=True)\n",
        "    for _, d, _ in data:\n",
        "        d = np.expand_dims(d, axis=-1)\n",
        "        x_val.append(d)\n",
        "\n",
        "x_train = np.array(x_train, dtype=np.float32)\n",
        "x_val = np.array(x_val, dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "encoder_model = tf.keras.models.load_model('/content/drive/MyDrive/GSoC/best_autoencoder_model_2.h5')\n",
        "\n",
        "y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val_encoded = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "encoded_shape = x_train_encoded.shape[1:]  # Excluding the sample dimension\n",
        "\n",
        "x_train_encoded_flattened = x_train_encoded.reshape(-1, np.prod(encoded_shape))\n",
        "x_val_encoded_flattened = x_val_encoded.reshape(-1, np.prod(encoded_shape))\n",
        "\n",
        "def get_improved_classifier(input_dim, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(units=256, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(units=128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(units=64, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(units=num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "improved_classifier_model = get_improved_classifier(\n",
        "    np.prod(encoded_shape), num_classes\n",
        ")\n",
        "\n",
        "# Train the classifier using flattened encoded representations and one-hot encoded labels\n",
        "improved_classifier_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = improved_classifier_model.fit(\n",
        "    x_train_encoded_flattened, y_train_encoded,\n",
        "    batch_size=batch_size,\n",
        "    epochs=30,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Evaluate the improved classifier's accuracy on the flattened validation encoded representations and one-hot encoded labels\n",
        "improved_classifier_accuracy = improved_classifier_model.evaluate(x_val_encoded_flattened, y_val_encoded)[1]\n",
        "print(\"Improved Classifier Accuracy:\", improved_classifier_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpD2tzU8KwHX",
        "outputId": "e386080f-a46a-48f2-8133-ef646e4f3f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "4862/4862 [==============================] - 27s 5ms/step - loss: 2.0995 - accuracy: 0.5220 - val_loss: 0.7321 - val_accuracy: 0.5056\n",
            "Epoch 2/30\n",
            "4862/4862 [==============================] - 22s 4ms/step - loss: 0.6888 - accuracy: 0.5260 - val_loss: 0.7203 - val_accuracy: 0.5141\n",
            "Epoch 3/30\n",
            "4862/4862 [==============================] - 24s 5ms/step - loss: 0.6847 - accuracy: 0.5338 - val_loss: 0.7080 - val_accuracy: 0.4939\n",
            "Epoch 4/30\n",
            "4862/4862 [==============================] - 23s 5ms/step - loss: 0.6870 - accuracy: 0.5292 - val_loss: 0.7338 - val_accuracy: 0.5080\n",
            "Epoch 5/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6871 - accuracy: 0.5308 - val_loss: 0.7280 - val_accuracy: 0.5196\n",
            "Epoch 6/30\n",
            "4862/4862 [==============================] - 24s 5ms/step - loss: 0.6851 - accuracy: 0.5342 - val_loss: 0.7428 - val_accuracy: 0.5128\n",
            "Epoch 7/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6856 - accuracy: 0.5322 - val_loss: 0.7836 - val_accuracy: 0.5068\n",
            "Epoch 8/30\n",
            "4862/4862 [==============================] - 24s 5ms/step - loss: 0.6853 - accuracy: 0.5324 - val_loss: 0.7314 - val_accuracy: 0.5080\n",
            "Epoch 9/30\n",
            "4862/4862 [==============================] - 23s 5ms/step - loss: 0.6838 - accuracy: 0.5311 - val_loss: 0.8196 - val_accuracy: 0.5135\n",
            "Epoch 10/30\n",
            "4862/4862 [==============================] - 25s 5ms/step - loss: 0.6862 - accuracy: 0.5317 - val_loss: 0.7522 - val_accuracy: 0.5079\n",
            "Epoch 11/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6843 - accuracy: 0.5352 - val_loss: 0.8609 - val_accuracy: 0.5126\n",
            "Epoch 12/30\n",
            "4862/4862 [==============================] - 23s 5ms/step - loss: 0.6852 - accuracy: 0.5324 - val_loss: 0.7976 - val_accuracy: 0.5134\n",
            "Epoch 13/30\n",
            "4862/4862 [==============================] - 22s 4ms/step - loss: 0.6854 - accuracy: 0.5362 - val_loss: 0.8240 - val_accuracy: 0.5039\n",
            "Epoch 14/30\n",
            "4862/4862 [==============================] - 22s 4ms/step - loss: 0.6862 - accuracy: 0.5332 - val_loss: 0.8093 - val_accuracy: 0.5047\n",
            "Epoch 15/30\n",
            "4862/4862 [==============================] - 23s 5ms/step - loss: 0.6846 - accuracy: 0.5340 - val_loss: 0.7915 - val_accuracy: 0.5129\n",
            "Epoch 16/30\n",
            "4862/4862 [==============================] - 23s 5ms/step - loss: 0.6848 - accuracy: 0.5343 - val_loss: 0.8267 - val_accuracy: 0.5125\n",
            "Epoch 17/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6833 - accuracy: 0.5353 - val_loss: 0.8425 - val_accuracy: 0.5047\n",
            "Epoch 18/30\n",
            "4862/4862 [==============================] - 24s 5ms/step - loss: 0.6864 - accuracy: 0.5350 - val_loss: 0.8954 - val_accuracy: 0.5123\n",
            "Epoch 19/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6819 - accuracy: 0.5341 - val_loss: 0.7823 - val_accuracy: 0.5134\n",
            "Epoch 20/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6839 - accuracy: 0.5337 - val_loss: 0.9573 - val_accuracy: 0.5133\n",
            "Epoch 21/30\n",
            "4862/4862 [==============================] - 24s 5ms/step - loss: 0.6859 - accuracy: 0.5322 - val_loss: 0.8732 - val_accuracy: 0.5116\n",
            "Epoch 22/30\n",
            "4862/4862 [==============================] - 21s 4ms/step - loss: 0.6862 - accuracy: 0.5318 - val_loss: 0.9652 - val_accuracy: 0.5086\n",
            "Epoch 23/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6865 - accuracy: 0.5330 - val_loss: 0.8626 - val_accuracy: 0.5136\n",
            "Epoch 24/30\n",
            "4862/4862 [==============================] - 24s 5ms/step - loss: 0.6928 - accuracy: 0.5320 - val_loss: 0.8814 - val_accuracy: 0.5088\n",
            "Epoch 25/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6855 - accuracy: 0.5320 - val_loss: 0.8100 - val_accuracy: 0.5049\n",
            "Epoch 26/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6838 - accuracy: 0.5304 - val_loss: 0.8777 - val_accuracy: 0.5169\n",
            "Epoch 27/30\n",
            "4862/4862 [==============================] - 23s 5ms/step - loss: 0.6828 - accuracy: 0.5323 - val_loss: 0.8956 - val_accuracy: 0.5128\n",
            "Epoch 28/30\n",
            "4862/4862 [==============================] - 21s 4ms/step - loss: 0.6907 - accuracy: 0.5326 - val_loss: 1.3466 - val_accuracy: 0.5086\n",
            "Epoch 29/30\n",
            "4862/4862 [==============================] - 22s 5ms/step - loss: 0.6858 - accuracy: 0.5323 - val_loss: 1.1716 - val_accuracy: 0.5098\n",
            "Epoch 30/30\n",
            "4862/4862 [==============================] - 22s 4ms/step - loss: 0.6838 - accuracy: 0.5319 - val_loss: 1.9352 - val_accuracy: 0.5064\n",
            "1717/1717 [==============================] - 6s 4ms/step - loss: 0.9840 - accuracy: 0.4727\n",
            "Improved Classifier Accuracy: 0.4726839065551758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhdXKknsapxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PREDICTION"
      ],
      "metadata": {
        "id": "HK0g4j4pPZMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_model = tf.keras.models.load_model('/content/drive/MyDrive/GSoC/best_autoencoder_model_2.h5')\n"
      ],
      "metadata": {
        "id": "anvUKO4Q-1uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "with open(\"/content/drive/MyDrive/GSoC/2014-11-11_0000_US_KNBC_The_Ellen_DeGeneres_Show_1930-2276_npy-detect_w5_p6_r0.025.npy\", \"rb\") as npf:\n",
        "    data = np.load(npf, allow_pickle=True)\n",
        "\n",
        "for item in data:\n",
        "    if isinstance(item, tuple) and len(item) >= 2:\n",
        "        frame = item[0]  # Extract the frame or label\n",
        "        d = item[1]      # Extract the data\n",
        "        x_val.append(np.array([d], dtype=np.float32))  # Assuming 'd' is the data\n",
        "        y_val.append(frame)  # Append the frame or label\n",
        "    else:\n",
        "        # Handle other cases where the structure of 'data' doesn't match expectations\n",
        "        pass\n",
        "\n",
        "x_val = np.array(x_val, dtype=np.float32)\n",
        "y_val = np.array(y_val, dtype=int)"
      ],
      "metadata": {
        "id": "61OUBRUN_KaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_test = []\n",
        "\n",
        "with open(\"/content/drive/MyDrive/GSoC/npy_files/2014-11-11_0000_US_KNBC_The_Ellen_DeGeneres_Show_1930-2276_npy-train_w5_p6_r0.025.npy\", \"rb\") as npf:\n",
        "    data = np.load(npf, allow_pickle=True)\n",
        "for frame, d, lb in data:\n",
        "    d = np.expand_dims(d, axis=-1)  # Add an additional dimension for the channel\n",
        "    x_test.append(d)\n",
        "\n",
        "x_test = np.array(x_test, dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "Rf-483drAg2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = autoencoder_model.predict(np.array(x_test, dtype=np.float32), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL8bW5jzAk0k",
        "outputId": "9bc5abaa-5771-4790-ead4-d519185cc677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "298/298 [==============================] - 11s 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for frame_data, prediction in zip(data, predictions):\n",
        "    frame = frame_data[0]  # Assuming frame data is a sequence with frame at index 0\n",
        "    d = frame_data[1]      # Assuming d is at index 1 in frame_data\n",
        "\n",
        "    results.append([frame, prediction[0] > .35])  # Use your desired threshold\n"
      ],
      "metadata": {
        "id": "0UTWDRCoA-JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"frame\", \"gesture\"])\n",
        "frames_with_gesture = df[df[\"gesture\"].apply(np.any)][\"frame\"].to_numpy()\n",
        "\n",
        "np.save('/content/drive/MyDrive/GSoC/frames_with_gesture-autoencoder.npy', frames_with_gesture)\n",
        "\n"
      ],
      "metadata": {
        "id": "tqaCymGdBHNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(frames_with_gesture)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Uqw0dDB6OY",
        "outputId": "b56e3dbb-c68a-452b-92da-e18f83ace260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9838"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}